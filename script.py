import os
import re
import base64
import requests
import csv
from io import StringIO
from urllib.parse import unquote
from github import Github
import random

# --- 1. é…ç½®è¯»å– ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
REPO_NAME = os.getenv("REPO_NAME")
FILE_PATH = os.getenv("FILE_PATH")
WEBPAGE_URLS = os.getenv("WEBPAGE_URLS", "").strip().splitlines()

COUNTRY_ORDER_STR = os.getenv("COUNTRY_ORDER") or ""
COUNTRY_ORDER = [code.strip() for code in COUNTRY_ORDER_STR.split(',')] if COUNTRY_ORDER_STR else []

LINKS_PER_COUNTRY = int(os.getenv("LINKS_PER_COUNTRY") or "20")
LINK_PREFIX = os.getenv("LINK_PREFIX") or ""
LINK_SUFFIX = os.getenv("LINK_SUFFIX") or ""

# æ–°å¢ï¼šè¾“å‡ºæ ¼å¼æ§åˆ¶å™¨ (simple/full)
OUTPUT_FORMAT = os.getenv("OUTPUT_FORMAT") or "full"

if not GITHUB_TOKEN or not REPO_NAME or not FILE_PATH: exit(1)
if not WEBPAGE_URLS: exit(1)

COUNTRY_MAPPING = {
    # äºšæ´²
    "é¦™æ¸¯": "HK", "æ¾³é—¨": "MO", "å°æ¹¾": "TW", "ä¸­å›½": "CN", "å¤§é™†": "CN",
    "æ—¥æœ¬": "JP", "éŸ©å›½": "KR", "æ–°åŠ å¡": "SG", "é©¬æ¥è¥¿äºš": "MY", "æ³°å›½": "TH",
    "è¶Šå—": "VN", "è²å¾‹å®¾": "PH", "å°åº¦å°¼è¥¿äºš": "ID", "å°åº¦": "IN", "å­ŸåŠ æ‹‰": "BD",
    "æŸ¬åŸ”å¯¨": "KH", "è€æŒ": "LA", "ç¼…ç”¸": "MM", "å°¼æ³Šå°”": "NP", "å·´åŸºæ–¯å¦": "PK",
    "æ–¯é‡Œå…°å¡": "LK", "åœŸè€³å…¶": "TR", "é˜¿è”é…‹": "AE", "æ²™ç‰¹": "SA", "ä»¥è‰²åˆ—": "IL",
    "å¡å¡”å°”": "QA", "å·´æ—": "BH", "ç§‘å¨ç‰¹": "KW", "é˜¿æ›¼": "OM", "æ ¼é²å‰äºš": "GE",
    "äºšç¾å°¼äºš": "AM", "é˜¿å¡æ‹œç–†": "AZ", "å“ˆè¨å…‹æ–¯å¦": "KZ", "å‰å°”å‰æ–¯æ–¯å¦": "KG",
    "å¡”å‰å…‹æ–¯å¦": "TJ", "ä¹Œå…¹åˆ«å…‹æ–¯å¦": "UZ",
    # æ¬§æ´²
    "è‹±å›½": "GB", "æ³•å›½": "FR", "å¾·å›½": "DE", "è·å…°": "NL", "ç‘å£«": "CH",
    "ä¿„ç½—æ–¯": "RU", "ä¹Œå…‹å…°": "UA", "æ„å¤§åˆ©": "IT", "è¥¿ç­ç‰™": "ES", "è‘¡è„ç‰™": "PT",
    "ç‘å…¸": "SE", "æŒªå¨": "NO", "ä¸¹éº¦": "DK", "èŠ¬å…°": "FI", "å†°å²›": "IS",
    "çˆ±å°”å…°": "IE", "æ¯”åˆ©æ—¶": "BE", "å¢æ£®å ¡": "LU", "å¥¥åœ°åˆ©": "AT", "æ³¢å…°": "PL",
    "æ·å…‹": "CZ", "åŒˆç‰™åˆ©": "HU", "ç½—é©¬å°¼äºš": "RO", "ä¿åŠ åˆ©äºš": "BG", "å¸Œè…Š": "GR",
    "çˆ±æ²™å°¼äºš": "EE", "æ‹‰è„±ç»´äºš": "LV", "ç«‹é™¶å®›": "LT", "ç™½ä¿„ç½—æ–¯": "BY", "æ‘©å°”å¤šç“¦": "MD",
    "å¡å°”ç»´äºš": "RS", "å…‹ç½—åœ°äºš": "HR", "æ–¯æ´›æ–‡å°¼äºš": "SI", "æ–¯æ´›ä¼å…‹": "SK",
    "æ³¢é»‘": "BA", "é»‘å±±": "ME", "é˜¿å°”å·´å°¼äºš": "AL", "åŒ—é©¬å…¶é¡¿": "MK", "é©¬è€³ä»–": "MT", "å¡æµ¦è·¯æ–¯": "CY",
    # åŒ—ç¾æ´²
    "ç¾å›½": "US", "åŠ æ‹¿å¤§": "CA", "å¢¨è¥¿å“¥": "MX",
    # å—ç¾æ´²
    "å·´è¥¿": "BR", "é˜¿æ ¹å»·": "AR", "å“¥ä¼¦æ¯”äºš": "CO", "æ™ºåˆ©": "CL", "ç§˜é²": "PE",
    # å¤§æ´‹æ´²
    "æ¾³å¤§åˆ©äºš": "AU", "æ–°è¥¿å…°": "NZ",
    # éæ´²
    "å—é": "ZA", "åŸƒåŠ": "EG", "å°¼æ—¥åˆ©äºš": "NG", "è‚¯å°¼äºš": "KE", "æ‘©æ´›å“¥": "MA",
}
CODE_TO_NAME = {v: k for k, v in COUNTRY_MAPPING.items()}
CODE_TO_FLAG = {
    # äºšæ´²
    "HK": "ğŸ‡­ğŸ‡°", "MO": "ğŸ‡²ğŸ‡´", "TW": "ğŸ‡¹ğŸ‡¼", "CN": "ğŸ‡¨ğŸ‡³", "JP": "ğŸ‡¯ğŸ‡µ", "KR": "ğŸ‡°ğŸ‡·",
    "SG": "ğŸ‡¸ğŸ‡¬", "MY": "ğŸ‡²ğŸ‡¾", "TH": "ğŸ‡¹ğŸ‡­", "VN": "ğŸ‡»ğŸ‡³", "PH": "ğŸ‡µğŸ‡­", "ID": "ğŸ‡®ğŸ‡©",
    "IN": "ğŸ‡®ğŸ‡³", "BD": "ğŸ‡§ğŸ‡©", "KH": "ğŸ‡°ğŸ‡­", "LA": "ğŸ‡±ğŸ‡¦", "MM": "ğŸ‡²ğŸ‡²", "NP": "ğŸ‡³ğŸ‡µ",
    "PK": "ğŸ‡µğŸ‡°", "LK": "ğŸ‡±ğŸ‡°", "TR": "ğŸ‡¹ğŸ‡·", "AE": "ğŸ‡¦ğŸ‡ª", "SA": "ğŸ‡¸ğŸ‡¦", "IL": "ğŸ‡®ğŸ‡±",
    "QA": "ğŸ‡¶ğŸ‡¦", "BH": "ğŸ‡§ğŸ‡­", "KW": "ğŸ‡°ğŸ‡¼", "OM": "ğŸ‡´ğŸ‡²", "GE": "ğŸ‡¬ğŸ‡ª", "AM": "ğŸ‡¦ğŸ‡²",
    "AZ": "ğŸ‡¦ğŸ‡¿", "KZ": "ğŸ‡°ğŸ‡¿", "KG": "ğŸ‡°ğŸ‡¬", "TJ": "ğŸ‡¹ğŸ‡¯", "UZ": "ğŸ‡ºğŸ‡¿",
    # æ¬§æ´²
    "GB": "ğŸ‡¬ğŸ‡§", "FR": "ğŸ‡«ğŸ‡·", "DE": "ğŸ‡©ğŸ‡ª", "NL": "ğŸ‡³ğŸ‡±", "CH": "ğŸ‡¨ğŸ‡­", "RU": "ğŸ‡·ğŸ‡º",
    "UA": "ğŸ‡ºğŸ‡¦", "IT": "ğŸ‡®ğŸ‡¹", "ES": "ğŸ‡ªğŸ‡¸", "PT": "ğŸ‡µğŸ‡¹", "SE": "ğŸ‡¸ğŸ‡ª", "NO": "ğŸ‡³ğŸ‡´",
    "DK": "ğŸ‡©ğŸ‡°", "FI": "ğŸ‡«ğŸ‡®", "IS": "ğŸ‡®ğŸ‡¸", "IE": "ğŸ‡®ğŸ‡ª", "BE": "ğŸ‡§ğŸ‡ª", "LU": "ğŸ‡±ğŸ‡º",
    "AT": "ğŸ‡¦ğŸ‡¹", "PL": "ğŸ‡µğŸ‡±", "CZ": "ğŸ‡¨ğŸ‡¿", "HU": "ğŸ‡­ğŸ‡º", "RO": "ğŸ‡·ğŸ‡´", "BG": "ğŸ‡§ğŸ‡¬",
    "GR": "ğŸ‡¬ğŸ‡·", "EE": "ğŸ‡ªğŸ‡ª", "LV": "ğŸ‡±ğŸ‡»", "LT": "ğŸ‡±ğŸ‡¹", "BY": "ğŸ‡§ğŸ‡¾", "MD": "ğŸ‡²ğŸ‡©",
    "RS": "ğŸ‡·ğŸ‡¸", "HR": "ğŸ‡­ğŸ‡·", "SI": "ğŸ‡¸ğŸ‡®", "SK": "ğŸ‡¸ğŸ‡°", "BA": "ğŸ‡§ğŸ‡¦", "ME": "ğŸ‡²ğŸ‡ª",
    "AL": "ğŸ‡¦ğŸ‡±", "MK": "ğŸ‡²ğŸ‡°", "MT": "ğŸ‡²ğŸ‡¹", "CY": "ğŸ‡¨ğŸ‡¾",
    # åŒ—ç¾æ´²
    "US": "ğŸ‡ºğŸ‡¸", "CA": "ğŸ‡¨ğŸ‡¦", "MX": "ğŸ‡²ğŸ‡½",
    # å—ç¾æ´²
    "BR": "ğŸ‡§ğŸ‡·", "AR": "ğŸ‡¦ğŸ‡·", "CO": "ğŸ‡¨ğŸ‡´", "CL": "ğŸ‡¨ğŸ‡±", "PE": "ğŸ‡µğŸ‡ª",
    # å¤§æ´‹æ´²
    "AU": "ğŸ‡¦ğŸ‡º", "NZ": "ğŸ‡³ğŸ‡¿",
    # éæ´²
    "ZA": "ğŸ‡¿ğŸ‡¦", "EG": "ğŸ‡ªğŸ‡¬", "NG": "ğŸ‡³ğŸ‡¬", "KE": "ğŸ‡°ğŸ‡ª", "MA": "ğŸ‡²ğŸ‡¦",
    # æœªçŸ¥
    "UNKNOWN": "â“"
}

# --- 3. é“¾æ¥æå–å‡½æ•° (æ— éœ€æ”¹åŠ¨) ---
def extract_vless_links(decoded_content):
    regex = re.compile(r'(vless|vmess)://[a-zA-Z0-9\-]+@([^:]+):(\d+)\?[^#]+#([^\n\r]+)')
    links = []
    for match in regex.finditer(decoded_content):
        ip, port, country_name_raw = match.group(2), match.group(3), unquote(match.group(4).strip())
        country_code = "UNKNOWN"
        # ä½¿ç”¨æ›´é•¿çš„åˆ—è¡¨è¿›è¡ŒåŒ¹é…
        for name, code in COUNTRY_MAPPING.items():
            if name in country_name_raw: country_code = code; break
        else:
            code_match = re.search(r'([A-Z]{2})', country_name_raw)
            if code_match: country_code = code_match.group(1)
        
        if country_code != "UNKNOWN":
            link_part = f"{ip}:{port}"
            links.append({"link_part": link_part, "code": country_code})
    return links

def extract_csv_links(csv_content):
    links = []
    f = StringIO(csv_content)
    reader = csv.reader(f)
    try:
        next(reader)
        for row in reader:
            if len(row) >= 4:
                ip, port, code = row[0].strip(), row[1].strip(), row[3].strip()
                if ip and port and code:
                    link_part = f"{ip}:{port}"
                    links.append({"link_part": link_part, "code": code})
    except Exception as e:
        print(f"  > CSV è§£ææ—¶å‡ºé”™: {e}")
    return links

def extract_line_based_links(plain_content):
    links = []
    for line in plain_content.strip().splitlines():
        clean_line = line.strip()
        if not clean_line: continue
        trojan_match = re.search(r'trojan://[^@]+@([^:]+):(\d+)[^#]*#(.+)', clean_line)
        if trojan_match:
            host, port, code = trojan_match.group(1), trojan_match.group(2), trojan_match.group(3).strip()
            link_part = f"{host}:{port}"
            links.append({"link_part": link_part, "code": code})
            continue
        ip_port_match = re.search(r'([^:]+:\d+)#(.+)', clean_line)
        if ip_port_match:
            link_part, code = ip_port_match.group(1), ip_port_match.group(2).strip()
            links.append({"link_part": link_part, "code": code})
    return links

# --- 4. æ ¸å¿ƒå¤„ç†é€»è¾‘ (æ— éœ€æ”¹åŠ¨) ---
def process_subscription_url(url):
    print(f"æ­£åœ¨å¤„ç† URL: {url}")
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        raw_content = response.text
        try:
            base64_content = "".join(raw_content.split())
            missing_padding = len(base64_content) % 4
            if missing_padding: base64_content += '=' * (4 - missing_padding)
            decoded_bytes = base64.b64decode(base64_content)
            decoded_text = decoded_bytes.decode('utf-8')
            print("  > æ£€æµ‹åˆ° Base64 æ ¼å¼ï¼Œä½¿ç”¨ vless/vmess è§£æå™¨ã€‚")
            return extract_vless_links(decoded_text)
        except Exception: pass
        first_line = raw_content.strip().splitlines()[0] if raw_content.strip() else ""
        if ',' in first_line and "åœ°å€" in first_line:
            print("  > æ£€æµ‹åˆ° CSV æ ¼å¼ï¼Œä½¿ç”¨ CSV è§£æå™¨ã€‚")
            return extract_csv_links(raw_content)
        print("  > æœªçŸ¥æ ¼å¼ï¼Œä½¿ç”¨é€šç”¨çš„è¡Œè§£æå™¨ã€‚")
        return extract_line_based_links(raw_content)
    except requests.RequestException as e:
        print(f"  > è·å– URL å†…å®¹å¤±è´¥: {e}")
        return []

def format_link(link_part, code, index=None):
    if OUTPUT_FORMAT == 'full':
        flag = CODE_TO_FLAG.get(code, CODE_TO_FLAG["UNKNOWN"])
        name = CODE_TO_NAME.get(code, code)
        name_suffix = str(index) if index is not None else ""
        return f"{link_part}#{LINK_PREFIX}{flag}{name}{name_suffix}{LINK_SUFFIX}"
    else:
        return f"{link_part}#{LINK_PREFIX}{code}{LINK_SUFFIX}"

def filter_and_sort_links(all_links, order, limit):
    grouped_links = {}
    for link_info in all_links:
        code = link_info['code']
        if code not in grouped_links: grouped_links[code] = []
        grouped_links[code].append(link_info['link_part'])
    
    order_to_use = order if order else list(grouped_links.keys())
    
    final_links = []
    for code in order_to_use:
        if code in grouped_links:
            unique_links = list(dict.fromkeys(grouped_links[code]))
            num_to_sample = min(limit, len(unique_links))
            randomly_selected = random.sample(unique_links, num_to_sample)
            
            for i, link_part in enumerate(randomly_selected, 1):
                final_links.append(format_link(link_part, code, i))
                
    return final_links

def write_to_github(content):
    if not content:
        print("æ²¡æœ‰ç”Ÿæˆä»»ä½•å†…å®¹ï¼Œå·²è·³è¿‡å†™å…¥ GitHubã€‚")
        return
    try:
        g = Github(GITHUB_TOKEN)
        repo = g.get_repo(REPO_NAME)
        try:
            file = repo.get_contents(FILE_PATH, ref="main")
            repo.update_file(path=FILE_PATH, message="Update subscription links", content=content, sha=file.sha, branch="main")
            print(f"æ–‡ä»¶ {FILE_PATH} å·²åœ¨ä»“åº“ {REPO_NAME} ä¸­æˆåŠŸæ›´æ–°ã€‚")
        except Exception:
            repo.create_file(path=FILE_PATH, message="Create subscription links file", content=content, branch="main")
            print(f"æ–‡ä»¶ {FILE_PATH} å·²åœ¨ä»“åº“ {REPO_NAME} ä¸­æˆåŠŸåˆ›å»ºã€‚")
    except Exception as e:
        print(f"å†™å…¥ GitHub æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# --- 5. ä¸»å‡½æ•° (æ— éœ€æ”¹åŠ¨) ---
def main():
    print("å¼€å§‹æ‰§è¡Œè®¢é˜…é“¾æ¥å¤„ç†ä»»åŠ¡...")
    all_extracted_links = []
    for url in WEBPAGE_URLS:
        if url:
            links = process_subscription_url(url)
            if links:
                all_extracted_links.extend(links)
    
    print(f"\nä»æ‰€æœ‰æºå…±æå–äº† {len(all_extracted_links)} ä¸ªæœ‰æ•ˆé“¾æ¥ã€‚")

    if not all_extracted_links:
        print("æœªèƒ½ä»ä»»ä½•æºæå–åˆ°é“¾æ¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return

    final_links = []
    if COUNTRY_ORDER:
        print("æ£€æµ‹åˆ° COUNTRY_ORDER, è¿›å…¥ã€éšæœºæ’åºã€‘åˆ†ç»„æ¨¡å¼...")
        final_links = filter_and_sort_links(all_extracted_links, COUNTRY_ORDER, LINKS_PER_COUNTRY)
    else:
        print("æœªæ£€æµ‹åˆ° COUNTRY_ORDER, è¿›å…¥åŸå§‹é¡ºåºæ¨¡å¼...")
        country_counters = {}
        for link_info in all_extracted_links:
            code = link_info['code']
            link_part = link_info['link_part']
            
            current_count = country_counters.get(code, 0) + 1
            country_counters[code] = current_count
            
            final_links.append(format_link(link_part, code, current_count))

    print(f"ç»è¿‡å¤„ç†åï¼Œæœ€ç»ˆä¿ç•™ {len(final_links)} ä¸ªé“¾æ¥ã€‚")
    final_content = "\n".join(final_links)
    write_to_github(final_content)

if __name__ == "__main__":
    main()
